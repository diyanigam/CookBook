{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyM8dm009GR02YAdWGFc7H6k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diyanigam/CookBook/blob/main/CB_API_hosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxPergJc-CFm"
      },
      "outputs": [],
      "source": [
        "!pip install transformers peft torch langchain langchain-community uvicorn fastapi ngrok pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from typing import Dict, Any\n",
        "from fastapi.middleware.cors import CORSMiddleware"
      ],
      "metadata": {
        "id": "YuQXJtyTpe5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "access_token = \"**********\"\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=access_token)\n",
        "model = PeftModel.from_pretrained(base_model, \"diyanigam/CookBook\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=access_token)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "print(f\"Model loaded successfully on {device}.\")"
      ],
      "metadata": {
        "id": "kTZBpfVinEMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "hf_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    temperature=0.8,\n",
        "    do_sample=True,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    device=0 if device == \"cuda\" else -1\n",
        ")\n",
        "\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
        "print(f\"Model loaded successfully on {device} and wrapped for LangChain.\")"
      ],
      "metadata": {
        "id": "2id2V90Jp4Zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = FastAPI()\n",
        "conversation_memories: Dict[str, ConversationBufferMemory] = {}\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"https://cook-book-phi.vercel.app\", \"http://localhost:5174/\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "class MessageRequest(BaseModel):\n",
        "    user_id: str\n",
        "    message: str\n",
        "\n",
        "@app.post(\"/chat/\")\n",
        "async def chat_with_model(request: MessageRequest):\n",
        "    user_id = request.user_id\n",
        "    user_message = request.message\n",
        "\n",
        "    # Get or create memory for the user\n",
        "    if user_id not in conversation_memories:\n",
        "        conversation_memories[user_id] = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "        print(f\"Created new memory for user: {user_id}\")\n",
        "\n",
        "    current_memory = conversation_memories[user_id]\n",
        "\n",
        "    template = \"\"\"You are a helpful and detailed recipe generator. You can remember past conversations.\n",
        "    Generate recipes and modify them based on user requests, making sure to include or exclude specific ingredients.\n",
        "\n",
        "    **Do NOT generate the next human turn in the conversation. Only provide your AI response.**\n",
        "\n",
        "    **Do NOT repeat the instructions or the previous conversation. Only provide the recipe.**\n",
        "\n",
        "\n",
        "    Current conversation:\n",
        "    {chat_history}\n",
        "    Human: {input}\n",
        "    AI:\"\"\"\n",
        "\n",
        "    prompt_template = PromptTemplate(input_variables=[\"chat_history\", \"input\"], template=template)\n",
        "    conversation_chain = ConversationChain(\n",
        "        llm=llm,\n",
        "        memory=current_memory,\n",
        "        prompt=prompt_template,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = conversation_chain.predict(input=user_message)\n",
        "        return {\"user_id\": user_id, \"response\": response}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n"
      ],
      "metadata": {
        "id": "z6INFAidyqf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!killall ngrok"
      ],
      "metadata": {
        "id": "KNuov1-AJrKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "!ngrok config add-authtoken **************\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "PORT = 8000\n",
        "\n",
        "try:\n",
        "    public_url = ngrok.connect(PORT).public_url\n",
        "    print(f\"ngrok Public URL: {public_url}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error starting ngrok: {e}\")\n",
        "    print(\"Please ensure ngrok is installed and you have a valid auth token if needed.\")\n",
        "\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=PORT)"
      ],
      "metadata": {
        "id": "ktYnEXpYyqmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eXVEUmPFscsl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}